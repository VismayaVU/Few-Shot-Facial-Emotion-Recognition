{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQeFIFyv1jXD"
   },
   "source": [
    "# **Real-Time Facial Expression Detection using Few-Shot Learning**\n",
    "\n",
    "Shri Hari S - PES1UG22AM154\n",
    "\n",
    "Venkat Subramanian - PES1UG22AM188\n",
    "\n",
    "Vishwanath Sridhar - PES1UG22AM194\n",
    "\n",
    "Vismaya Vadana - PES1UG22AM195"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qqofWHC1jXF"
   },
   "source": [
    "Kaggle Data Source to Import FER2013 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FyLr39poezrT",
    "outputId": "08aacda7-a1ca-4da3-a704-aa021d328d31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/msambare/fer2013?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60.3M/60.3M [00:00<00:00, 76.9MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source import complete.\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
    "# THEN FEEL FREE TO DELETE THIS CELL.\n",
    "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
    "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
    "# NOTEBOOK.\n",
    "import kagglehub\n",
    "msambare_fer2013_path = kagglehub.dataset_download('msambare/fer2013')\n",
    "\n",
    "print('Data source import complete.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70rJeXW0gL0e",
    "outputId": "4038c438-b584-4506-c1fb-52384793634b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of the dataset directory:\n",
      "['train', 'test']\n",
      "Train directory contents:\n",
      "['sad', 'angry', 'fear', 'disgust', 'surprise', 'neutral', 'happy']\n",
      "Test directory contents:\n",
      "['sad', 'angry', 'fear', 'disgust', 'surprise', 'neutral', 'happy']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# List the contents of the downloaded dataset directory\n",
    "dataset_dir = msambare_fer2013_path\n",
    "print(\"Contents of the dataset directory:\")\n",
    "print(os.listdir(dataset_dir))\n",
    "\n",
    "# If you want to load the images, specify the path to the train and test folders\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "test_dir = os.path.join(dataset_dir, 'test')\n",
    "\n",
    "# List the emotion subdirectories in the training and testing sets\n",
    "print(\"Train directory contents:\")\n",
    "print(os.listdir(train_dir))\n",
    "\n",
    "print(\"Test directory contents:\")\n",
    "print(os.listdir(test_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Z8E_Vkw3a2W"
   },
   "source": [
    "**Installing EasyFSL**\n",
    "\n",
    "The `easyfsl` library simplifies the implementation of few-shot learning pipelines by providing ready-to-use modules for tasks like Prototypical Networks, Matching Networks, and Relation Networks. It includes utilities for dataset management, task sampling, and model evaluation in few-shot learning scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xX0huUQ11pQS"
   },
   "source": [
    "Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "DjqEHy_fezrU",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import resnet18\n",
    "from tqdm import tqdm\n",
    "from easyfsl.samplers import TaskSampler\n",
    "from easyfsl.utils import plot_images, sliding_average\n",
    "import cv2\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "train_dir = 'train'\n",
    "test_dir = 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzElrycC1twA"
   },
   "source": [
    "Defining the constants for training the Meta-Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6hVTPvN4fhFH"
   },
   "outputs": [],
   "source": [
    "# Define constants\n",
    "image_size = 48  # Change this to match the FER2013 image size\n",
    "N_WAY = 5  # Number of classes in a task\n",
    "N_SHOT = 10  # Number of images per class in the support set\n",
    "N_QUERY = 15  # Number of images per class in the query set\n",
    "N_EVALUATION_TASKS = 100\n",
    "N_TRAINING_EPISODES = 60000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zy8YjW7z1y0h"
   },
   "source": [
    "Data Augmentation and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kQp819fyfije"
   },
   "outputs": [],
   "source": [
    "# First, let's correct the transforms\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),  # Convert to tensor first\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                       std=[0.229, 0.224, 0.225])  # Then normalize\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9cgiIK0xfkM2"
   },
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_set = ImageFolder(root=train_dir, transform=train_transforms)\n",
    "test_set = ImageFolder(root=test_dir, transform=test_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bI93jD_x2EmQ"
   },
   "source": [
    "Creating Task Samplers for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UweRDpgqflkv"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_set.get_labels = lambda: [instance[1] for instance in train_set.samples]\n",
    "train_sampler = TaskSampler(\n",
    "    train_set, n_way=N_WAY, n_shot=N_SHOT, n_query=N_QUERY, n_tasks=N_TRAINING_EPISODES\n",
    ")\n",
    "\n",
    "test_set.get_labels = lambda: [instance[1] for instance in test_set.samples]\n",
    "test_sampler = TaskSampler(\n",
    "    test_set, n_way=N_WAY, n_shot=N_SHOT, n_query=N_QUERY, n_tasks=N_EVALUATION_TASKS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "b_IP3iYEfoCu"
   },
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_sampler=train_sampler,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    collate_fn=train_sampler.episodic_collate_fn,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    batch_sampler=test_sampler,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    collate_fn=test_sampler.episodic_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIxWtYSp2Lh5"
   },
   "source": [
    "**Prototypical Networks Architecture**\n",
    "\n",
    "\n",
    "Implements Prototypical Networks with a backbone for feature extraction and a projection head for dimensionality reduction, computing class prototypes and classification scores based on Euclidean distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OxivTiRffprX"
   },
   "outputs": [],
   "source": [
    "class PrototypicalNetworks(nn.Module):\n",
    "    def __init__(self, backbone: nn.Module):\n",
    "        super(PrototypicalNetworks, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(512, 256),  # Assuming ResNet18's output is 512\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "\n",
    "    def forward(self, support_images, support_labels, query_images):\n",
    "        z_support = self.projection(self.backbone(support_images))\n",
    "        z_query = self.projection(self.backbone(query_images))\n",
    "\n",
    "        # Infer the number of different classes from the labels of the support set\n",
    "        n_way = len(torch.unique(support_labels))\n",
    "        # Prototype i is the mean of all instances of features corresponding to labels == i\n",
    "        z_proto = torch.cat(\n",
    "            [z_support[support_labels == label].mean(0).unsqueeze(0) for label in range(n_way)]\n",
    "        )\n",
    "\n",
    "        # Compute the euclidean distance from queries to prototypes\n",
    "        dists = torch.cdist(z_query, z_proto)\n",
    "\n",
    "        # Transform distances into classification scores\n",
    "        scores = -dists\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jGHFz1v2alq"
   },
   "source": [
    "**Model Initialization**\n",
    "\n",
    "Initialize the Prototypical Networks model using a ResNet18 backbone pretrained on ImageNet, with the fully connected layer replaced by a flattening operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "n_P-5w-5frTn"
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "convolutional_network = resnet18(weights='IMAGENET1K_V1')\n",
    "convolutional_network.fc = nn.Flatten()\n",
    "model = PrototypicalNetworks(convolutional_network)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmcR0S5A2kaQ"
   },
   "source": [
    "**Evaluation Function**\n",
    "\n",
    "Defines a task-based evaluation process for Prototypical Networks, calculating accuracy across multiple tasks using support and query sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6VsR59EBfuAn",
    "outputId": "0ed03060-8394-466e-e37c-7e04f1cb50cf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:11<00:00,  8.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model tested on 100 tasks. Accuracy: 29.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function\n",
    "def evaluate_on_one_task(support_images: torch.Tensor, support_labels: torch.Tensor,\n",
    "                         query_images: torch.Tensor, query_labels: torch.Tensor) -> (int, int):\n",
    "    return (\n",
    "        torch.max(model(support_images.cuda(), support_labels.cuda(), query_images.cuda()).detach(), 1)[1]\n",
    "        == query_labels.cuda()\n",
    "    ).sum().item(), len(query_labels)\n",
    "\n",
    "def evaluate(data_loader: DataLoader):\n",
    "    total_predictions = 0\n",
    "    correct_predictions = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for support_images, support_labels, query_images, query_labels, _ in tqdm(data_loader):\n",
    "            correct, total = evaluate_on_one_task(support_images, support_labels, query_images, query_labels)\n",
    "            total_predictions += total\n",
    "            correct_predictions += correct\n",
    "\n",
    "    print(f\"Model tested on {len(data_loader)} tasks. Accuracy: {(100 * correct_predictions / total_predictions):.2f}%\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3g9SOfu2tV4"
   },
   "source": [
    "**Training and Validation Loop**\n",
    "\n",
    "Implements the training loop for Prototypical Networks with early stopping, a cosine annealing learning rate scheduler, and periodic validation for model performance monitoring and checkpoint saving.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZCUQucBhh40F",
    "outputId": "27f0288d-fd31-4ca7-e83c-7e7021081882"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60000 [00:00<?, ?it/s, loss=1.61, lr=0.0003]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy at episode 0: 27.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/60000 [00:08<55:42:13,  3.34s/it, loss=1.61, lr=0.0003] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best accuracy: 27.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1000/60000 [02:36<2:12:20,  7.43it/s, loss=0.997, lr=0.0003]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy at episode 1000: 57.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1002/60000 [02:44<22:56:50,  1.40s/it, loss=0.997, lr=0.0003]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best accuracy: 57.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1999/60000 [05:11<2:03:38,  7.82it/s, loss=0.885, lr=0.000299]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy at episode 2000: 61.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2002/60000 [05:18<16:56:29,  1.05s/it, loss=0.885, lr=0.000299]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best accuracy: 61.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 3000/60000 [07:46<2:02:24,  7.76it/s, loss=0.774, lr=0.000298]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy at episode 3000: 65.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 3002/60000 [07:53<18:25:22,  1.16s/it, loss=0.774, lr=0.000298]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best accuracy: 65.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 3999/60000 [10:22<1:57:09,  7.97it/s, loss=0.677, lr=0.000297]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy at episode 4000: 67.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 4002/60000 [10:30<15:42:17,  1.01s/it, loss=0.677, lr=0.000297]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best accuracy: 67.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4999/60000 [12:59<2:01:07,  7.57it/s, loss=0.763, lr=0.000295]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy at episode 5000: 68.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 5002/60000 [13:06<18:33:48,  1.22s/it, loss=0.763, lr=0.000295]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best accuracy: 68.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 6002/60000 [15:45<17:01:18,  1.13s/it, loss=0.748, lr=0.000293]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy at episode 6000: 67.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6999/60000 [18:14<1:55:28,  7.65it/s, loss=0.612, lr=0.00029]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy at episode 7000: 68.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 7002/60000 [18:23<19:18:54,  1.31s/it, loss=0.612, lr=0.00029]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best accuracy: 68.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 8002/60000 [21:00<19:05:51,  1.32s/it, loss=0.598, lr=0.000287]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy at episode 8000: 68.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 9000/60000 [23:29<1:47:41,  7.89it/s, loss=0.573, lr=0.000284]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy at episode 9000: 68.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 9002/60000 [23:38<18:21:01,  1.30s/it, loss=0.573, lr=0.000284]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best accuracy: 68.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 10003/60000 [26:14<15:28:36,  1.11s/it, loss=0.536, lr=0.00028]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy at episode 10000: 68.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 11003/60000 [28:52<13:51:34,  1.02s/it, loss=0.492, lr=0.000276]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy at episode 11000: 67.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 11999/60000 [31:22<1:41:15,  7.90it/s, loss=0.512, lr=0.000271]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy at episode 12000: 69.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 12002/60000 [31:31<16:24:08,  1.23s/it, loss=0.512, lr=0.000271]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best accuracy: 69.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 13003/60000 [34:09<15:05:34,  1.16s/it, loss=0.433, lr=0.000267]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy at episode 13000: 68.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 13999/60000 [36:39<1:34:11,  8.14it/s, loss=0.414, lr=0.000262]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy at episode 14000: 69.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 14003/60000 [36:48<13:35:17,  1.06s/it, loss=0.414, lr=0.000262]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best accuracy: 69.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 14999/60000 [39:19<2:18:23,  5.42it/s, loss=0.463, lr=0.000256]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy at episode 15000: 69.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 15002/60000 [39:28<14:56:14,  1.20s/it, loss=0.463, lr=0.000256]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best accuracy: 69.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 16003/60000 [42:05<13:42:12,  1.12s/it, loss=0.427, lr=0.000251]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy at episode 16000: 67.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 17002/60000 [44:43<17:47:14,  1.49s/it, loss=0.336, lr=0.000245]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy at episode 17000: 69.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 17999/60000 [47:12<1:39:04,  7.07it/s, loss=0.328, lr=0.000238]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy at episode 18000: 70.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 18002/60000 [47:21<13:57:35,  1.20s/it, loss=0.328, lr=0.000238]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best accuracy: 70.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 19003/60000 [49:58<11:09:17,  1.02it/s, loss=0.281, lr=0.000232]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy at episode 19000: 69.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 20003/60000 [52:33<12:36:11,  1.13s/it, loss=0.288, lr=0.000225]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy at episode 20000: 67.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 21003/60000 [55:10<14:33:15,  1.34s/it, loss=0.225, lr=0.000218]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy at episode 21000: 68.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 22001/60000 [57:46<14:36:14,  1.38s/it, loss=0.278, lr=0.000211]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy at episode 22000: 70.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 23000/60000 [1:00:13<1:19:51,  7.72it/s, loss=0.201, lr=0.000204]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy at episode 23000: 71.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 23002/60000 [1:00:21<13:34:27,  1.32s/it, loss=0.201, lr=0.000204]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best accuracy: 71.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 24002/60000 [1:02:58<11:17:27,  1.13s/it, loss=0.148, lr=0.000197]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy at episode 24000: 69.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 25002/60000 [1:05:32<9:49:51,  1.01s/it, loss=0.236, lr=0.000189] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy at episode 25000: 69.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 26002/60000 [1:08:06<8:04:11,  1.17it/s, loss=0.168, lr=0.000182]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy at episode 26000: 70.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 27003/60000 [1:10:42<8:03:46,  1.14it/s, loss=0.144, lr=0.000174] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy at episode 27000: 69.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 28000/60000 [1:13:17<1:23:45,  6.37it/s, loss=0.137, lr=0.000166]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy at episode 28000: 69.69%\n",
      "Early stopping triggered!\n",
      "\n",
      "Final evaluation with best model (acc: 71.29%):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "<ipython-input-31-ee19d09b9781>:90: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('best_model.pth')\n"
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def fit(support_images: torch.Tensor, support_labels: torch.Tensor,\n",
    "        query_images: torch.Tensor, query_labels: torch.Tensor) -> float:\n",
    "    optimizer.zero_grad()\n",
    "    classification_scores = model(support_images.cuda(), support_labels.cuda(), query_images.cuda())\n",
    "    loss = criterion(classification_scores, query_labels.cuda())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "# Now let's set up the complete training loop with validation\n",
    "# Initialize variables for early stopping\n",
    "best_acc = 0\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "# Create scheduler\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0003, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                T_max=N_TRAINING_EPISODES,\n",
    "                                                eta_min=1e-6)\n",
    "\n",
    "# Training loop\n",
    "log_update_frequency = 10\n",
    "all_loss = []\n",
    "model.train()\n",
    "\n",
    "with tqdm(enumerate(train_loader), total=len(train_loader)) as tqdm_train:\n",
    "    for episode_index, (support_images, support_labels, query_images, query_labels, _) in tqdm_train:\n",
    "        # Training step\n",
    "        loss_value = fit(support_images, support_labels, query_images, query_labels)\n",
    "        all_loss.append(loss_value)\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # Update progress bar\n",
    "        if episode_index % log_update_frequency == 0:\n",
    "            tqdm_train.set_postfix(\n",
    "                loss=sliding_average(all_loss, log_update_frequency),\n",
    "                lr=scheduler.get_last_lr()[0]\n",
    "            )\n",
    "\n",
    "        # Validation step\n",
    "        if episode_index % 1000 == 0:\n",
    "            model.eval()\n",
    "            current_acc = 0\n",
    "            n_tasks = 0\n",
    "\n",
    "            # Compute validation accuracy\n",
    "            with torch.no_grad():\n",
    "                for val_support_images, val_support_labels, val_query_images, val_query_labels, _ in test_loader:\n",
    "                    correct, total = evaluate_on_one_task(\n",
    "                        val_support_images,\n",
    "                        val_support_labels,\n",
    "                        val_query_images,\n",
    "                        val_query_labels\n",
    "                    )\n",
    "                    current_acc += correct\n",
    "                    n_tasks += total\n",
    "\n",
    "            current_acc = (100 * current_acc / n_tasks)\n",
    "            print(f\"\\nValidation accuracy at episode {episode_index}: {current_acc:.2f}%\")\n",
    "\n",
    "            # Early stopping check\n",
    "            if current_acc > best_acc:\n",
    "                best_acc = current_acc\n",
    "                torch.save({\n",
    "                    'episode': episode_index,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'accuracy': best_acc,\n",
    "                }, 'best_model.pth')\n",
    "                patience_counter = 0\n",
    "                print(f\"New best accuracy: {best_acc:.2f}%\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered!\")\n",
    "                break\n",
    "\n",
    "            model.train()\n",
    "\n",
    "# Load best model and evaluate\n",
    "checkpoint = torch.load('best_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"\\nFinal evaluation with best model (acc: {checkpoint['accuracy']:.2f}%):\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jNCEkhPvqXSv",
    "outputId": "3825d4b1-b4f0-4bd9-ae20-e9e7a1195b7b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 11.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model tested on 100 tasks. Accuracy: 69.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "evaluate(test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhvzSx9m23Qm"
   },
   "source": [
    "**Model Loading and Final Evaluation**\n",
    "\n",
    "Dynamically sets the device (CPU or CUDA), loads the best model checkpoint, and prepares for final evaluation with the stored accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jf8abF5JAwvq",
    "outputId": "ba976056-a5c6-4b90-a6e1-24eddcc494b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final evaluation with best model (acc: 71.29%):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vishwanath Sridhar\\AppData\\Local\\Temp\\ipykernel_20200\\3953788035.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('model.pth', map_location=device)  # Map to the appropriate device\n"
     ]
    }
   ],
   "source": [
    "# Dynamically set the device to CPU or CUDA if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load best model and evaluate\n",
    "checkpoint = torch.load('model.pth', map_location=device)  # Map to the appropriate device\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"\\nFinal evaluation with best model (acc: {checkpoint['accuracy']:.2f}%):\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQz0o8f43IN6"
   },
   "source": [
    "**Live Face Recognition with Prototypical Networks**\n",
    "\n",
    "Implements a real-time face recognition system using a webcam, leveraging Prototypical Networks to classify faces based on embeddings and prototypes with smoothed predictions for robust inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "HKjAD6_2_xE1",
    "outputId": "3288cd30-9269-4d28-c787-6828a29671ae"
   },
   "outputs": [],
   "source": [
    "# Load pre-trained face detector (Haar Cascade or MTCNN)\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Preprocessing for the query image (face detection)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Ensure resizing to match the input size used during training\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Standard normalization\n",
    "])\n",
    "\n",
    "\n",
    "# Create a balanced support set with a max of 100 images per class\n",
    "class_indices = {}\n",
    "for idx, (_, label) in enumerate(train_set.imgs):\n",
    "    if label not in class_indices:\n",
    "        class_indices[label] = []\n",
    "    class_indices[label].append(idx)\n",
    "\n",
    "# Limit to a maximum of 100 images per class\n",
    "balanced_indices = []\n",
    "for label, indices in class_indices.items():\n",
    "    balanced_indices.extend(random.sample(indices, min(100, len(indices))))\n",
    "\n",
    "balanced_train_set = Subset(train_set, balanced_indices)\n",
    "train_loader = DataLoader(balanced_train_set, batch_size=8, shuffle=True)\n",
    "\n",
    "# Initialize variables to store the support set embeddings and prototypes\n",
    "support_embeddings = None\n",
    "prototypes = None\n",
    "\n",
    "# Start video capture from the webcam\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "if not video_capture.isOpened():\n",
    "    print(\"Error: Could not access webcam.\")\n",
    "    exit()\n",
    "\n",
    "# Variable to keep track of predictions over time (smoothing)\n",
    "previous_predictions = []\n",
    "prediction_buffer_size = 10\n",
    "\n",
    "# Smooth prediction using a moving average or majority vote\n",
    "def smooth_prediction(predictions, buffer_size):\n",
    "    return np.bincount(predictions[-buffer_size:]).argmax()\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Could not read frame.\")\n",
    "        break\n",
    "\n",
    "    # Convert frame to grayscale for the detector\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces\n",
    "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    # If faces are detected\n",
    "    for (x, y, w, h) in faces:\n",
    "        face = frame[y:y + h, x:x + w]\n",
    "\n",
    "        # Convert numpy.ndarray (OpenCV) to PIL.Image\n",
    "        face_image = Image.fromarray(cv2.cvtColor(face, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        # Apply transformation\n",
    "        face_tensor = transform(face_image).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "        # Get a small batch of images as the support set (only once per video loop)\n",
    "        if support_embeddings is None:  # Initialize the support set embeddings and prototypes\n",
    "            support_images, support_labels = next(iter(train_loader))  # Get the first batch\n",
    "            support_images, support_labels = support_images.to(device), support_labels.to(device)\n",
    "\n",
    "            # Extract embeddings for the support set\n",
    "            with torch.no_grad():\n",
    "                support_embeddings = model.backbone(support_images)  # Backbone returns the feature embeddings\n",
    "                support_embeddings = model.projection(support_embeddings)\n",
    "\n",
    "                # Calculate prototypes for the support set (mean of support embeddings per class)\n",
    "                prototypes = []\n",
    "                for label in torch.unique(support_labels):\n",
    "                    class_embeddings = support_embeddings[support_labels == label]\n",
    "                    prototypes.append(class_embeddings.mean(0).unsqueeze(0))\n",
    "                prototypes = torch.cat(prototypes)\n",
    "\n",
    "        # Forward pass through the model for query image (live face)\n",
    "        with torch.no_grad():\n",
    "            query_embedding = model.backbone(face_tensor)\n",
    "            query_embedding = model.projection(query_embedding)\n",
    "\n",
    "            # Calculate distances between query embedding and support prototypes (e.g., Euclidean distance)\n",
    "            distances = F.pairwise_distance(query_embedding, prototypes)\n",
    "            predicted_class = distances.argmin().item()\n",
    "\n",
    "            # Append the current prediction to previous_predictions for smoothing\n",
    "            previous_predictions.append(predicted_class)\n",
    "\n",
    "            # Use the moving average of the last 10 predictions for smoothing\n",
    "            smoothed_prediction = smooth_prediction(previous_predictions, prediction_buffer_size)\n",
    "\n",
    "            # Display the smoothed predicted class on the live video\n",
    "            cv2.putText(frame, f\"Predicted Class: {smoothed_prediction}\", (x, y - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "        # Draw rectangle around the detected face\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Video - Face Detection with Model Inference', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and close all OpenCV windows\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
